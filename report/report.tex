\documentclass{article}

\input{defs}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\title{Bootstrapped MPC\\\large CS 294-112}
\author{Vladimir Feinberg, Samvit Jain, Michael Whittaker}
\date{10 October 2017}

\begin{document}
	
\maketitle

\section{Introduction}

We changed our research direction from the encoded states research to exploring model predictive control (MPC), namely a method we call bootstrapped MPC (BMPC). BMPC is a model-based algorithm that can be viewed as instantiation of the meta-MPC algorithm, which also explains the motivation for bootstrapped MPC (Algorithm~\ref{alg:metampc}). Note the (continuous) domains for actions and states are $\mcA, \mcS$ and our restriction to deterministic dynamics and rewards for simplicity (though extensions would not require much modification).

\begin{algorithm}
\caption{The \textsc{MetaMPC} algorithm is a template for MPC-based algorithms. As template parameters, it accepts a number of simulations to perform $K$ and the simulation horizon $H$. The critical template parameter of interest is the time-dependent action-sampling distribution $A_t$. This is a stochastic policy, returning an action $a$ for a provided state $s$.}\label{alg:metampc}
\begin{algorithmic}[1]
\Procedure{MetaMPC}{state $s$, dynamics $f:\mcS\times\mcA\rightarrow\mcS$, reward $r:\mcS\times\mcA\times \mcS\rightarrow\R$}
\State $\ca{s_1^{(i)}\gets s}_{i=1}^K$.
\For { $i\gets 1, \ldots, K$}
\For { $t\gets 1\,\ldots,H$}
\State sample $a_t^{(i)}\sim A_t(s_t)$
\State $s_{t+1}^{(i)}\gets f\pa{s_t^{(i)},a_t^{(i)}}$
\EndFor
\State $R_i\gets\sum_{t=1}^{H}r\pa{s_{t}^{(i)},a_t^{(i)},s_{t+1}^{(i)}}$
\EndFor
\State $i_*=\argmax_i R_i$
\State \Return $a_1^{(i_*)}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

We believe \textsc{MetaMPC} works when $R_i$ is a decent estimator for the true $H$-step reward of the agent under the \textsc{MetaMPC} policy, supposing the agent takes $a_1^{(i)}$ now and continues with \textsc{MetaMPC} later. Then, choosing the first action $a_1^{(i_*)}$ that results in the best $R_i$ would be the smartest move one could make. Regular MPC specifies $A_t(s)=\Uniform(\mcA)$ for a rectangle $\mcA$, independent of $t,s$. But a uniform distribution is a poor representation of future \textsc{MetaMPC} steps: it stands to reason that a more intelligent selection of $A_t$ might improve performance.

In particular, we consider learning a (mostly) stationary action distribution conditional on the state, $\pi_\theta(s)$, which is trained to mimic the result of the MPC algorithm. In particular, every iteration, we roll out \textsc{MetaMPC} with $A_t=\pi_\theta^{(t)}$ (with $\theta$ held fixed during both the real and simulated rollouts). After collecting some rollouts, resulting in a large dataset of state-action pairs $(s,a, s')\in\mcD$, we train $\pi_\theta^{(t)}$ to replicate \textsc{MetaMPC}'s behavior with the expectation that the reward estimates improve (let this be BMPC). We can view this as bootstrapping performance because as $\pi_\theta$ gets more accurate, the BMPC policy is able to take better actions, which are then learned by $\pi$, resulting in a virtuous cycle. The dynamics are learned in a similar manner with the dataset of transitions.

\section{Related Work}

To some degree, this approach is similar to other continuous-action methods that seek to optimize the reward function directly, since in effect we are proposing a method for stochastic optimization of the rollout reward. One difference than, say, CMA-ES, would be that we explicitly model the reinforcement learning setting by modelling the BMPC agent as a stationary policy.

To some degree this approach is similar to Alpha-Go style sampling, where instead of replicating agent behavior the goal is to have intelligent MCTS pruning for a UCT. If UCT pruning is guided by a neural network, then one might view UCT as a tree-version of this BMPC approach (where the space for exploration, being discrete, is much more tractable to explore in the exhaustive tree manner).

\section{Easy Examples}

We consider learning the dynamics for the \texttt{HalfCheetah-v1} environment and using those learned dynamics for BMPC planning. We note two important difference from the regular \texttt{gym} environment: the full observation is provided so the dynamics and rewards are true functions of the previous state (the usual observation excerpts the first coordinate of the agent).

Unless otherwise specified, all parameters are their defaults: 3 different seeds for each setup, 60 epochs of dynamics training on the full dataset per on-policy iteration, a dynamics batch size of 512, maximum episode length of 1000, 1000 simulated paths in the MPC controller (each of which has a simulated horizon of $H$ 15), and 10 paths sampled by the initial random agent and then by the MPC controller that are aggregated every policy iteration. The learning rate was the default $10^{-3}$. Each such policy iteration contains a round of training the learner policy $\pi_\theta$ in BMPC. All policies $\pi_\theta$ are neural networks with $\dim \mcA$ outputs and $\dim\mcS$ inputs, all depth 5, width 32, trained with 100 epochs after each policy iteration with 512-size minibatches and a $10^{-3}$ learning rate.

We consider a couple variants of BMPC, which are all defined by the conditional action distribution $A_t$. While our conditional distributions are mostly stationary, we consider exploring at the first step $t=1$. Equation~\ref{eq:delta} describes the $\delta$-BMPC, with a point mass describing the deterministic action in the \textsc{MetaMPC} simulation taken by $A_t$ when $t>1$. $\delta$-BMPC is trained on minimizing the loss $\ell$, which is the MSE from the expert.
\begin{align} \label{eq:delta}
  A_t(s)=\begin{cases}
\Uniform\pa{\mcA}    & t=1 \\
\delta_{\pi_\theta(s)} & \text{otherwise}
  \end{cases} && \ell(\theta)=\sum_{(s, a)\in\mcD}\norm{\pi_\theta(s)-a}_2^2
\end{align}

Gaussian BMPC relaxes the determinism of $\delta$-BMPC, fitting a conditional diagonal Gaussian to the \textsc{MetaMPC} actions, as described in Equation~\ref{eq:stochastic}.
\begin{align} \label{eq:stochastic}
  A_t(s)=\begin{cases}
\Uniform\pa{\mcA}    & t=1 \\
N\pa{\pi_\theta(s), \diag\vsigma^2} & \text{otherwise}
  \end{cases} && \ell(\theta,\vsigma)=-\sum_{(s, a)\in\mcD}\log p_{\theta,\vsigma}(a|s)
\end{align}

No-explore BMPC reduces the exploration by keeping the policy purely conditional diagonal Gaussian and stationary, with no initial exploration, as described in Equation~\ref{eq:no-explore}.
\begin{align} \label{eq:no-explore}
  A_t(s)= N\pa{\pi_\theta(s), \diag\vsigma^2} && \ell(\theta)=\sum_{(s, a)\in\mcD}\norm{\pi_\theta(s)-a}_2^2
\end{align}

We evaluate these examples with an easy, supervised reward for \texttt{HalfCheetah-v1} which was provided in HW4 (\figref{easy}). Note that this is different than the classical \texttt{gym} reward.

\autofig{easy-AverageReturn.pdf}{\label{fig:easy}}{Average return over policy aggregation iterations for the learning-based sampling MPC agent, compared to the standard uniform sampling MPC agent.}

In \figref{easy}, we see a couple interesting things going on already. Amazingly, the improvement above happens even though the learners themselves are not near the MPC performance at any iteration (\figref{easy-learners}).

\autofig{easy-LearnerAverageReturn.pdf}{\label{fig:easy-learners}}{Average return over policy aggregation iterations for an agent sampling from $A_2$ for each model, along with the original MPC as a baseline.}

The poor learner performance does not kill the original hypothesis: perhaps even weak learners can improve reward estimates $R_{i_*}$. But this does suggest an new hypothesis for the improvement: perhaps the learners prevent catastrophic changes in the policy. Maybe simply reducing the variance of estimates $R_{i_*}$ yields the improvement. We note that, at least for this simple case, learners don't improve performance by making the dynamics more learnable: for both the original MPC and BMPC, the learned dynamics are about equally performant (\figref{easy-dynamics}.

\autofig{easy-DynamicsMSE.pdf}{\label{fig:easy-dynamics}}{We evaluate dynamics in the setting of \figref{easy}. This evaluation is measures how predictive the dynamics were of the actual transitions in validation rollouts (it is not the training MSE). We omit the first couple of iterations because their scales are vastly different than the rest.}

\section{Hard Cost}

If we force ourselves to only rely on the usual \texttt{HalfCheetah-v1} reward, which only rewards forward movement and discourages large-magnitude actions, we are faced with a more difficult setting.

In this case, it is unclear if bootstrapping still helps (\figref{hardcost}).

\autofig{hard-AverageReturn.pdf}{\label{fig:hardcost}}{Average return over policy aggregation iterations for the learning-based sampling MPC agent, compared to the standard uniform sampling MPC agent, this time with a hard cost function.}

The dynamics and learning returns for the runs in \figref{hardcost} paint a similar picture as for easy cost. In case hard cost required additional simulation, we changed $H,K$ from $15,1000$ to $50,1000$. Performance was generally better but more chaotic, and still with no clear advantage to BMPC (\figref{hard2cost}).

\autofig{hard2-AverageReturn.pdf}{\label{fig:hard2cost}}{The same setting as \figref{hard2cost} but with longer simulated horizons.}

Nonetheless, the trend in \figref{hardcost} deserves elaboration---perhaps at later iterations BMPC overtakes MPC.

\section{Planned Work}

The proposition being investigated (more accurate or lower-variance estimates of reward with $R_{i_*}$) can be directly sampled and tested to be the case! After simulating several rollouts for any \textsc{MetaMPC} algorithm, we can sample initial state $s$ from the empirical distribution of visited states. For each such state, we \textit{re-simulate} $H$ real horizon steps with independent runs (the samples $A_t$ will differ, so the results will not be the same as the first run, even for a deterministic environment). These give us several observations of the true $H$-step reward $\tilde{R}_H$. For each such observation we compute the observed bias $\tilde{R}_H-R_{i_*}$ and squared error $\pa{\tilde{R}_H-R_{i_*}}^2$. Over several samples we can estimate variance $\var R_{i_*}$ as well. Then across the samples $s$ we can estimate average values across the entire distribution for these reward statistics.

This sampling of $s$ is not iid but the resulting estimator is still unbiased, so we will have to use the bootstrap (haha) to calculate confidence intervals.

Measuring the explicit action distribution shift between iterations boils down to measuring distance between conditional action distributions resulting from the \textsc{MetaMPC} algorithm, which is only a black-box generative distribution. We have a plan for this but excerpt it to keep the report shorter. Graders can contact the authors for more details.

In addition, we hope to expand to the harder \texttt{Ant} problem to prevent overfitting to \texttt{HalfCheetah}.

Finally, the chaotic performance but improving learner in the second hard setting suggest that we should explore various hyperparameter and template settings; but we should save this for the end.

\section{Further Exploration}

We also explored changing the initial exploration policy $A_1$ from $\Uniform(\mcA)$ to $N(\pi_\theta, \lambda I)$. This resulted in some improvements, but we leave these out of the report for simplicity (graders may contact the authors directly for these results). Further, to improve how quickly $\pi_\theta$ learns expert actions, we consider DAgger iteration with $\pi_\theta$ as the learner and \textsc{MetaMPC} as the expert, but managing the learned dynamics for both the learner and expert will require additional work.

\end{document}
